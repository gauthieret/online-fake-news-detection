{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Kaggle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# fake_csv = \"../online-fake-news-detection/data/DS1_Fake.csv\"\n",
    "# true_csv = \"../online-fake-news-detection/data/DS1_True.csv\"\n",
    "# fake_df = pd.read_csv(fake_csv)\n",
    "# true_df = pd.read_csv(true_csv)\n",
    "# true_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafilatura\n",
    "downloaded = trafilatura.fetch_url('https://edition.cnn.com/europe/live-news/russia-ukraine-war-news-11-29-22/index.html')\n",
    "extract = trafilatura.extract(downloaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GH Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS3_submit_csv = \"../ofnd/data/DS3_submit.csv\"\n",
    "DS3_submit_df = pd.read_csv(DS3_submit_csv)\n",
    "DS3_test_csv = \"../ofnd/data/DS3_test.csv\"\n",
    "DS3_test_df = pd.read_csv(DS3_test_csv)\n",
    "DS3_submit_test_df = pd.merge(DS3_submit_df,DS3_test_df,'inner')\n",
    "DS3_train_csv = \"../ofnd/data/DS3_train.csv\"\n",
    "DS3_train_df = pd.read_csv(DS3_train_csv)\n",
    "DS3_df = pd.merge(DS3_submit_test_df,DS3_train_df, 'outer')\n",
    "DS3_df['news'] = DS3_df['title'] + DS3_df['text']\n",
    "DS3_df = DS3_df.drop(columns = ['id','author','title','text'])\n",
    "DS3_df['label'] = DS3_df['label'].apply(lambda x: True if x == 1 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26000, 2)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DS3_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DS2_csv = \"../ofnd/data/DS2_fakenews.csv\"\n",
    "DS2_df = pd.read_csv(DS2_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10240, 2)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DS2_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS4_csv = \"../ofnd/data/DS4.csv\"\n",
    "DS4_df = pd.read_csv(DS4_csv)\n",
    "DS4_df['news'] = DS4_df['title'] + DS4_df['text']\n",
    "DS4_df = DS4_df.drop(columns = ['Unnamed: 0','title','text'])\n",
    "DS4_df['label'] = DS4_df['label'].apply(lambda x: True if x == 1 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72134, 2)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DS4_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.merge(DS2_df,DS3_df, 'outer')\n",
    "articles_df = pd.merge(articles_df, DS4_df,'outer')\n",
    "#articles_df = DS4_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing NAN rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = articles_df[articles_df['news'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = articles_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "\n",
    "    \n",
    "    # Basic cleaning\n",
    "    \n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercase \n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "    \n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "\n",
    "    unaccented_string = unidecode.unidecode(sentence) # remove accents\n",
    "\n",
    "    tokenized_sentence = word_tokenize(unaccented_string) ## tokenize \n",
    "    stop_words = set(stopwords.words('english')) ## define stopwords\n",
    "\n",
    "    tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "    w for w in tokenized_sentence if not w in stop_words\n",
    "            ]\n",
    "\n",
    "    lemmatized = [\n",
    "    WordNetLemmatizer().lemmatize(word, pos = \"v\") \n",
    "    for word in tokenized_sentence_cleaned\n",
    "    ]\n",
    "    \n",
    "    cleaned_sentence = ' '.join(word for word in lemmatized)\n",
    "\n",
    "    \n",
    "    return cleaned_sentence\n",
    "\n",
    "articles_df['cleaned_news'] = articles_df['news'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_y split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = 'label'\n",
    "feature = articles_df['cleaned_news']\n",
    "\n",
    "def X_y(df, TARGET_COLUMN):\n",
    "    X = df.drop([TARGET_COLUMN], axis=1)\n",
    "    y = df[TARGET_COLUMN]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_data(X, y):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\\\n",
    "        test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X, y = X_y(articles_df, target)\n",
    "X_train, X_test, y_train, y_test = split_data(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit & Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import numpy as np\n",
    "X_train_cleaned_title_text = X_train['cleaned_news']\n",
    "X_test_cleaned_title_text = X_test['cleaned_news']\n",
    "\n",
    "def train_vect(X: np.ndarray):\n",
    "    tfidf_vectorizer = CountVectorizer(binary=False, decode_error='strict', encoding='utf-8',\n",
    "                                input='content', lowercase=True, max_df=1.0,\n",
    "                                max_features=None, min_df=1,\n",
    "                                ngram_range=(1, 1), preprocessor=None,\n",
    "                                stop_words=None, strip_accents=None,\n",
    "                                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "                                tokenizer=None, vocabulary=None)\n",
    "                                \n",
    "    tfidf_fitted = tfidf_vectorizer.fit(X)\n",
    "    \n",
    "    return tfidf_fitted\n",
    "\n",
    "\n",
    "def transform_vect(X: np.ndarray, tfdidf_fitted):\n",
    "    tfidf_transformed = tfdidf_fitted.transform(X)\n",
    "\n",
    "    return tfidf_transformed\n",
    "\n",
    "\n",
    "tfdidf_fitted = train_vect(X_train_cleaned_title_text)\n",
    "X_train_vectorized = transform_vect(X_train_cleaned_title_text, tfdidf_fitted)\n",
    "X_test_vectorized = transform_vect(X_test_cleaned_title_text, tfdidf_fitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<23390x302285 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4075714 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized\n",
    "X_test_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate : Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# Cross-validation\n",
    "cv_results = cross_validate(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
    "                             X_train_vectorized, y_train, cv=5, scoring=[\"accuracy\"])\n",
    "average_accuracy = cv_results[\"test_accuracy\"].mean()\n",
    "np.round(average_accuracy,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.18787217, 0.12364697, 0.11828303, 0.12178707, 0.11660886]),\n",
       " 'score_time': array([0.04387593, 0.03363299, 0.03432083, 0.03082991, 0.03074694]),\n",
       " 'test_accuracy': array([0.82052222, 0.81768209, 0.81658268, 0.81410902, 0.80987722])}"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('online-fake-news-detection')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0b61e090d0ce072d1ebe263efbdfced32abe751f0ff09162c12e4f661763592"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
