from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from GEofnd.interface.main import predict
import json
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)
# $WIPE_BEGIN
# ðŸ’¡ Preload the model to accelerate the predictions
# We want to avoid loading the heavy deep-learning model from MLflow at each `get("/predict")`
# The trick is to load the model in memory when the uvicorn server starts
# Then to store the model in an `app.state.model` global variable accessible across all routes!
# This will prove very useful for demo days
# $WIPE_END

# http://127.0.0.1:8000/predict?pickup_datetime=2012-10-06 12:10:20&pickup_longitude=40.7614327&pickup_latitude=-73.9798156&dropoff_longitude=40.6513111&dropoff_latitude=-73.8803331&passenger_count=2
@app.get("/pred")
def pred(text_or_url):

    #    check if textorurl is text; or url
    # .    if url, get the text

    prediction = predict(text_or_url)
    return {
        "text_or_url": text_or_url,
        "prediction": prediction,
        }

